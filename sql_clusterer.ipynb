{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows sql clustering using embedding and vector database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Breparing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.0.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: pyarrow==14.0.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (14.0.1)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2024.3.1)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 5)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 5)) (0.5.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raohai/projj/github.com/RaoHai/sql_cluster/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/raohai/projj/github.com/RaoHai/sql_cluster/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           id                 domain  \\\n",
       "0       5097               forestry   \n",
       "1       5098       defense industry   \n",
       "2       5099         marine biology   \n",
       "3       5100     financial services   \n",
       "4       5101                 energy   \n",
       "...      ...                    ...   \n",
       "99995  89651              nonprofit   \n",
       "99996  89652                 retail   \n",
       "99997  89653       fitness industry   \n",
       "99998  89654      space exploration   \n",
       "99999  89655  wildlife conservation   \n",
       "\n",
       "                                      domain_description    sql_complexity  \\\n",
       "0      Comprehensive data on sustainable forest manag...       single join   \n",
       "1      Defense contract data, military equipment main...       aggregation   \n",
       "2      Comprehensive data on marine species, oceanogr...         basic SQL   \n",
       "3      Detailed financial data including investment s...       aggregation   \n",
       "4      Energy market data covering renewable energy s...  window functions   \n",
       "...                                                  ...               ...   \n",
       "99995  Nonprofit data on charitable giving trends, so...         basic SQL   \n",
       "99996  Retail data on circular supply chains, ethical...       single join   \n",
       "99997  Workout data, membership demographics, wearabl...       single join   \n",
       "99998  Spacecraft manufacturing data, space mission r...       single join   \n",
       "99999  Animal population data, habitat preservation e...         basic SQL   \n",
       "\n",
       "                              sql_complexity_description  \\\n",
       "0            only one join (specify inner, outer, cross)   \n",
       "1      aggregation functions (COUNT, SUM, AVG, MIN, M...   \n",
       "2               basic SQL with a simple select statement   \n",
       "3      aggregation functions (COUNT, SUM, AVG, MIN, M...   \n",
       "4      window functions (e.g., ROW_NUMBER, LEAD, LAG,...   \n",
       "...                                                  ...   \n",
       "99995           basic SQL with a simple select statement   \n",
       "99996        only one join (specify inner, outer, cross)   \n",
       "99997        only one join (specify inner, outer, cross)   \n",
       "99998        only one join (specify inner, outer, cross)   \n",
       "99999           basic SQL with a simple select statement   \n",
       "\n",
       "                 sql_task_type  \\\n",
       "0      analytics and reporting   \n",
       "1      analytics and reporting   \n",
       "2      analytics and reporting   \n",
       "3      analytics and reporting   \n",
       "4      analytics and reporting   \n",
       "...                        ...   \n",
       "99995  analytics and reporting   \n",
       "99996  analytics and reporting   \n",
       "99997  analytics and reporting   \n",
       "99998  analytics and reporting   \n",
       "99999  analytics and reporting   \n",
       "\n",
       "                               sql_task_type_description  \\\n",
       "0      generating reports, dashboards, and analytical...   \n",
       "1      generating reports, dashboards, and analytical...   \n",
       "2      generating reports, dashboards, and analytical...   \n",
       "3      generating reports, dashboards, and analytical...   \n",
       "4      generating reports, dashboards, and analytical...   \n",
       "...                                                  ...   \n",
       "99995  generating reports, dashboards, and analytical...   \n",
       "99996  generating reports, dashboards, and analytical...   \n",
       "99997  generating reports, dashboards, and analytical...   \n",
       "99998  generating reports, dashboards, and analytical...   \n",
       "99999  generating reports, dashboards, and analytical...   \n",
       "\n",
       "                                              sql_prompt  \\\n",
       "0      What is the total volume of timber sold by eac...   \n",
       "1      List all the unique equipment types and their ...   \n",
       "2      How many marine species are found in the South...   \n",
       "3      What is the total trade value and average pric...   \n",
       "4      Find the energy efficiency upgrades with the h...   \n",
       "...                                                  ...   \n",
       "99995  Which programs had the highest volunteer parti...   \n",
       "99996  What is the number of fair-trade certified acc...   \n",
       "99997  Find the user with the longest workout session...   \n",
       "99998  How many space missions were completed by each...   \n",
       "99999  Determine the number of unique animal species ...   \n",
       "\n",
       "                                             sql_context  \\\n",
       "0      CREATE TABLE salesperson (salesperson_id INT, ...   \n",
       "1      CREATE TABLE equipment_maintenance (equipment_...   \n",
       "2      CREATE TABLE marine_species (name VARCHAR(50),...   \n",
       "3      CREATE TABLE trade_history (id INT, trader_id ...   \n",
       "4      CREATE TABLE upgrades (id INT, cost FLOAT, typ...   \n",
       "...                                                  ...   \n",
       "99995  CREATE TABLE programs (program_id INT, num_vol...   \n",
       "99996  CREATE TABLE products (product_id INT, product...   \n",
       "99997  CREATE TABLE workout_sessions (id INT, user_id...   \n",
       "99998  CREATE TABLE SpaceMissions (id INT, astronaut_...   \n",
       "99999  CREATE TABLE animal_population (id INT, animal...   \n",
       "\n",
       "                                                     sql  \\\n",
       "0      SELECT salesperson_id, name, SUM(volume) as to...   \n",
       "1      SELECT equipment_type, SUM(maintenance_frequen...   \n",
       "2      SELECT COUNT(*) FROM marine_species WHERE loca...   \n",
       "3      SELECT trader_id, stock, SUM(price * quantity)...   \n",
       "4      SELECT type, cost FROM (SELECT type, cost, ROW...   \n",
       "...                                                  ...   \n",
       "99995  SELECT program_id, (num_volunteers / total_par...   \n",
       "99996  SELECT COUNT(*) FROM products WHERE is_fair_tr...   \n",
       "99997  SELECT u.name, MAX(session_duration) as max_du...   \n",
       "99998  SELECT a.name, COUNT(sm.id) FROM Astronauts a ...   \n",
       "99999  SELECT COUNT(DISTINCT animal_species) AS uniqu...   \n",
       "\n",
       "                                         sql_explanation  \n",
       "0      Joins timber_sales and salesperson tables, gro...  \n",
       "1      This query groups the equipment_maintenance ta...  \n",
       "2      This query counts the number of marine species...  \n",
       "3      This query calculates the total trade value an...  \n",
       "4      The SQL query uses the ROW_NUMBER function to ...  \n",
       "...                                                  ...  \n",
       "99995  This query calculates the participation rate f...  \n",
       "99996  The query counts the number of fair-trade cert...  \n",
       "99997  The query joins the workout_sessions and users...  \n",
       "99998  This query calculates the number of space miss...  \n",
       "99999  This query calculates the number of unique ani...  \n",
       "\n",
       "[100000 rows x 11 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'synthetic_text_to_sql_train.snappy.parquet', 'test': 'synthetic_text_to_sql_test.snappy.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/gretelai/synthetic_text_to_sql/\" + splits[\"train\"])\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"feature-extraction\", model=\"microsoft/codebert-base\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: (1, 12, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "sql_query = \"SELECT name, age FROM users WHERE age > 30\"\n",
    "embeddings = pipe(sql_query)\n",
    "\n",
    "embedding_size = np.array(embeddings).shape\n",
    "print(f\"Embedding size: {embedding_size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FAISS as Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_sql_queries=0    SELECT salesperson_id, name, SUM(volume) as to...\n",
      "1    SELECT equipment_type, SUM(maintenance_frequen...\n",
      "2    SELECT COUNT(*) FROM marine_species WHERE loca...\n",
      "3    SELECT trader_id, stock, SUM(price * quantity)...\n",
      "4    SELECT type, cost FROM (SELECT type, cost, ROW...\n",
      "5    SELECT SUM(spending) FROM defense.eu_humanitar...\n",
      "6    SELECT SpeciesName, AVG(WaterTemp) as AvgTemp ...\n",
      "7    DELETE FROM Program_Outcomes WHERE program_id ...\n",
      "8    SELECT SUM(fare) FROM bus_routes WHERE route_n...\n",
      "9    SELECT AVG(Property_Size) FROM Inclusive_Housi...\n",
      "Name: sql, dtype: object, text_dict={0: 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;', 1: 'SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;', 2: \"SELECT COUNT(*) FROM marine_species WHERE location = 'Southern Ocean';\", 3: 'SELECT trader_id, stock, SUM(price * quantity) as total_trade_value, AVG(price) as avg_price FROM trade_history GROUP BY trader_id, stock;', 4: 'SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;', 5: 'SELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;', 6: 'SELECT SpeciesName, AVG(WaterTemp) as AvgTemp FROM SpeciesWaterTemp INNER JOIN FishSpecies ON SpeciesWaterTemp.SpeciesID = FishSpecies.SpeciesID WHERE MONTH(Date) = 2 GROUP BY SpeciesName;', 7: 'DELETE FROM Program_Outcomes WHERE program_id = 1002;', 8: \"SELECT SUM(fare) FROM bus_routes WHERE route_name = 'Green Line';\", 9: \"SELECT AVG(Property_Size) FROM Inclusive_Housing WHERE Inclusive = 'Yes';\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 25.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "d = 768  # 向量维度\n",
    "\n",
    "top_sql_queries = df.head(10)[\"sql\"]  # 假设字段名称是 sql_query\n",
    "\n",
    "# 使用一个字典来存储原文与其对应的索引\n",
    "text_dict = {i: top_sql_queries[i] for i in range(len(top_sql_queries))}\n",
    "print(f\"top_sql_queries={top_sql_queries}, text_dict={text_dict}\")\n",
    "\n",
    "index = faiss.IndexFlatL2(d)  # 使用 L2 距离\n",
    "\n",
    "for sql_query in tqdm(top_sql_queries):\n",
    "  embeddings = pipe(sql_query)\n",
    "  # 转换嵌入为 NumPy 数组并计算平均值（句子的整体 embedding）\n",
    "  embeddings = np.array(embeddings[0])  # 获取第一个元素，即每个 token 的 embedding\n",
    "  sentence_embedding = embeddings.mean(axis=0)  # 对所有 token 的 embedding 取平均\n",
    "  \n",
    "  index.add(np.expand_dims(sentence_embedding, axis=0).astype('float32'))  # 添加到 FAISS 索引中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to retrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (1, 125, 768)\n",
      "Shape of sentence embedding: (1, 768)\n",
      "Shape of query vector for FAISS: (1, 768)\n",
      "query=\n",
      "  SELECT    SUM(volume) AS total_volume,\n",
      "            salesperson_id,\n",
      "            name\n",
      "  FROM      timber_sales\n",
      "  JOIN      salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id\n",
      "  GROUP BY  salesperson_id,\n",
      "            name\n",
      "  ORDER BY  total_volume DESC;\n",
      "\n",
      "SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "SELECT trader_id, stock, SUM(price * quantity) as total_trade_value, AVG(price) as avg_price FROM trade_history GROUP BY trader_id, stock;\n",
      "SELECT SpeciesName, AVG(WaterTemp) as AvgTemp FROM SpeciesWaterTemp INNER JOIN FishSpecies ON SpeciesWaterTemp.SpeciesID = FishSpecies.SpeciesID WHERE MONTH(Date) = 2 GROUP BY SpeciesName;\n",
      "SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;\n",
      "SELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经加载了数据并取得了前 1 条 SQL 查询\n",
    "xq = \"\"\"\n",
    "  SELECT    SUM(volume) AS total_volume,\n",
    "            salesperson_id,\n",
    "            name\n",
    "  FROM      timber_sales\n",
    "  JOIN      salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id\n",
    "  GROUP BY  salesperson_id,\n",
    "            name\n",
    "  ORDER BY  total_volume DESC;\n",
    "\"\"\"\n",
    "\n",
    "# 计算 SQL 查询的 embeddings\n",
    "xq_embeddings = pipe(xq)\n",
    "xq_embeddings = np.array(xq_embeddings)\n",
    "\n",
    "# 确保 embeddings 的形状为 (1, num_tokens, embedding_dim)\n",
    "print(f\"Shape of embeddings: {xq_embeddings.shape}\")  # 形状应该是 (1, 63, 768)\n",
    "\n",
    "# 对所有 token embeddings 取平均，得到句子的 embedding\n",
    "xq_sentence_embedding = xq_embeddings.mean(axis=1)  # 对维度 1 取平均，得到 (1, 768)\n",
    "\n",
    "# 确认句子的 embedding 形状，应该是 (768,)\n",
    "print(f\"Shape of sentence embedding: {xq_sentence_embedding.shape}\")  # 应该是 (768,)\n",
    "\n",
    "# 将句子的 embedding 转换为 (1, 768) 形式\n",
    "xq_sentence_vector = xq_sentence_embedding.astype('float32')  # 不需要 expand_dims，因为它已经是二维的\n",
    "\n",
    "# 确保索引的维度与 embedding 的维度匹配\n",
    "print(f\"Shape of query vector for FAISS: {xq_sentence_vector.shape}\")  # 应该是 (1, 768)\n",
    "\n",
    "# 查询索引，返回最相似的 5 个向量\n",
    "D, I = index.search(xq_sentence_vector, 5)\n",
    "\n",
    "print(f\"query={xq}\")\n",
    "# 打印最相似的向量的原文\n",
    "for idx in I[0]:\n",
    "    print(f\"{text_dict[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Try Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 46.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 1000 points in 768D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "centroids shape before reshape: (7680,) objective=5795.19 imbalance=1.227 nsplit=0       \n",
      "聚类中心：\n",
      "[[-0.18499173  0.22511667  0.13517778 ... -0.54662466 -0.41021362\n",
      "   0.46190667]\n",
      " [-0.28051266  0.08221427  0.15986638 ... -0.44513115 -0.38535556\n",
      "   0.4418672 ]\n",
      " [-0.29708812  0.06968737  0.14393795 ... -0.46478567 -0.41295123\n",
      "   0.46540022]\n",
      " ...\n",
      " [-0.33051264  0.0879201   0.20769997 ... -0.47243038 -0.46141124\n",
      "   0.5047192 ]\n",
      " [-0.33388335  0.08140443  0.16814938 ... -0.49328902 -0.46472928\n",
      "   0.45314988]\n",
      " [-0.34249762  0.11567847  0.23195948 ... -0.5568997  -0.44863892\n",
      "   0.46856833]]\n",
      "  Iteration 19 (0.01 s, search 0.01 s): objective=5793.7 imbalance=1.222 nsplit=0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 设置聚类的簇数\n",
    "k = 10  # 假设我们想要10个簇\n",
    "d = 768  # 向量维度\n",
    "\n",
    "top_sql_queries = df.head(1000)[\"sql\"]  # 假设字段名称是 sql_query\n",
    "\n",
    "clustering = faiss.Clustering(d, k)\n",
    "\n",
    "# 设置聚类的参数（例如，最大迭代次数）\n",
    "clustering.niter = 20  # 设置最大迭代次数\n",
    "clustering.max_points_per_centroid = 1000  # 每个质心的最大点数\n",
    "clustering.verbose = True  # 输出详细信息\n",
    "\n",
    "# 使用一个字典来存储原文与其对应的索引\n",
    "text_dict = {i: top_sql_queries[i] for i in range(len(top_sql_queries))}\n",
    "\n",
    "index = faiss.IndexFlatL2(d)  # 使用 L2 距离\n",
    "\n",
    "vectors = []\n",
    "for sql_query in tqdm(top_sql_queries):\n",
    "  embeddings = pipe(sql_query)\n",
    "  # 转换嵌入为 NumPy 数组并计算平均值（句子的整体 embedding）\n",
    "  embeddings = np.array(embeddings[0])  # 获取第一个元素，即每个 token 的 embedding\n",
    "  sentence_embedding = embeddings.mean(axis=0)  # 对所有 token 的 embedding 取平均\n",
    "  \n",
    "  vector = np.expand_dims(sentence_embedding, axis=0).astype('float32')\n",
    "  vectors.append(vector)\n",
    "\n",
    "# 将所有向量堆叠成一个大的数组\n",
    "vectors = np.vstack(vectors)  # (num_samples, d)\n",
    "\n",
    "# 执行聚类训练\n",
    "clustering.train(vectors, index)  # 进行聚类训练\n",
    "\n",
    "# 获取聚类中心\n",
    "centroids = faiss.vector_float_to_array(clustering.centroids)  # 获取聚类中心\n",
    "print(f\"centroids shape before reshape: {centroids.shape}\")\n",
    "\n",
    "# 检查是否可以 reshape\n",
    "if centroids.shape[0] == k * d:\n",
    "    centroids = centroids.reshape(k, d)  # reshape 成 k 个簇中心，每个簇中心是 d 维\n",
    "    print(f\"聚类中心：\\n{centroids}\")\n",
    "else:\n",
    "    print(f\"聚类中心的形状不符合预期，实际大小为 {centroids.shape}\")\n",
    "\n",
    "# 添加向量到索引\n",
    "index.add(vectors)  # 聚类后添加向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "簇 0 的原文：SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "簇 1 的原文：SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;\n",
      "簇 2 的原文：SELECT COUNT(*) FROM marine_species WHERE location = 'Southern Ocean';\n",
      "簇 3 的原文：SELECT trader_id, stock, SUM(price * quantity) as total_trade_value, AVG(price) as avg_price FROM trade_history GROUP BY trader_id, stock;\n",
      "簇 4 的原文：SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;\n",
      "簇 5 的原文：SELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;\n",
      "簇 6 的原文：SELECT SpeciesName, AVG(WaterTemp) as AvgTemp FROM SpeciesWaterTemp INNER JOIN FishSpecies ON SpeciesWaterTemp.SpeciesID = FishSpecies.SpeciesID WHERE MONTH(Date) = 2 GROUP BY SpeciesName;\n",
      "簇 7 的原文：DELETE FROM Program_Outcomes WHERE program_id = 1002;\n",
      "簇 8 的原文：SELECT SUM(fare) FROM bus_routes WHERE route_name = 'Green Line';\n",
      "簇 9 的原文：SELECT AVG(Property_Size) FROM Inclusive_Housing WHERE Inclusive = 'Yes';\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 获取每个簇中心对应的原文\n",
    "centroid_to_text = {}  # 用来存储每个簇中心对应的原文\n",
    "\n",
    "# 对每个簇中心，找到最接近的向量及其原文\n",
    "for i in range(k):\n",
    "    # 找到与第 i 个簇中心最接近的向量\n",
    "    centroid_vector = np.expand_dims(centroids[i], axis=0).astype('float32')  # 当前簇中心\n",
    "    distances, indices = index.search(centroid_vector, 1)  # 查找最近的一个向量\n",
    "    \n",
    "    # 获取该簇中心对应的原文\n",
    "    closest_index = indices[0][0]  # 获取最近的向量索引\n",
    "    closest_text = text_dict[closest_index]  # 获取该向量的原文\n",
    "    \n",
    "    # 将簇中心和对应的原文存储到字典中\n",
    "    centroid_to_text[i] = closest_text\n",
    "\n",
    "# 打印每个簇中心对应的原文\n",
    "for cluster_id, text in centroid_to_text.items():\n",
    "    print(f\"簇 {cluster_id} 的原文：{text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
