{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows sql clustering using embedding and vector database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Breparing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.0.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: pyarrow==14.0.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (14.0.1)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2024.3.1)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 5)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 5)) (0.5.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 4)) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raohai/projj/github.com/RaoHai/sql_cluster/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/raohai/projj/github.com/RaoHai/sql_cluster/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           id                 domain  \\\n",
       "0       5097               forestry   \n",
       "1       5098       defense industry   \n",
       "2       5099         marine biology   \n",
       "3       5100     financial services   \n",
       "4       5101                 energy   \n",
       "...      ...                    ...   \n",
       "99995  89651              nonprofit   \n",
       "99996  89652                 retail   \n",
       "99997  89653       fitness industry   \n",
       "99998  89654      space exploration   \n",
       "99999  89655  wildlife conservation   \n",
       "\n",
       "                                      domain_description    sql_complexity  \\\n",
       "0      Comprehensive data on sustainable forest manag...       single join   \n",
       "1      Defense contract data, military equipment main...       aggregation   \n",
       "2      Comprehensive data on marine species, oceanogr...         basic SQL   \n",
       "3      Detailed financial data including investment s...       aggregation   \n",
       "4      Energy market data covering renewable energy s...  window functions   \n",
       "...                                                  ...               ...   \n",
       "99995  Nonprofit data on charitable giving trends, so...         basic SQL   \n",
       "99996  Retail data on circular supply chains, ethical...       single join   \n",
       "99997  Workout data, membership demographics, wearabl...       single join   \n",
       "99998  Spacecraft manufacturing data, space mission r...       single join   \n",
       "99999  Animal population data, habitat preservation e...         basic SQL   \n",
       "\n",
       "                              sql_complexity_description  \\\n",
       "0            only one join (specify inner, outer, cross)   \n",
       "1      aggregation functions (COUNT, SUM, AVG, MIN, M...   \n",
       "2               basic SQL with a simple select statement   \n",
       "3      aggregation functions (COUNT, SUM, AVG, MIN, M...   \n",
       "4      window functions (e.g., ROW_NUMBER, LEAD, LAG,...   \n",
       "...                                                  ...   \n",
       "99995           basic SQL with a simple select statement   \n",
       "99996        only one join (specify inner, outer, cross)   \n",
       "99997        only one join (specify inner, outer, cross)   \n",
       "99998        only one join (specify inner, outer, cross)   \n",
       "99999           basic SQL with a simple select statement   \n",
       "\n",
       "                 sql_task_type  \\\n",
       "0      analytics and reporting   \n",
       "1      analytics and reporting   \n",
       "2      analytics and reporting   \n",
       "3      analytics and reporting   \n",
       "4      analytics and reporting   \n",
       "...                        ...   \n",
       "99995  analytics and reporting   \n",
       "99996  analytics and reporting   \n",
       "99997  analytics and reporting   \n",
       "99998  analytics and reporting   \n",
       "99999  analytics and reporting   \n",
       "\n",
       "                               sql_task_type_description  \\\n",
       "0      generating reports, dashboards, and analytical...   \n",
       "1      generating reports, dashboards, and analytical...   \n",
       "2      generating reports, dashboards, and analytical...   \n",
       "3      generating reports, dashboards, and analytical...   \n",
       "4      generating reports, dashboards, and analytical...   \n",
       "...                                                  ...   \n",
       "99995  generating reports, dashboards, and analytical...   \n",
       "99996  generating reports, dashboards, and analytical...   \n",
       "99997  generating reports, dashboards, and analytical...   \n",
       "99998  generating reports, dashboards, and analytical...   \n",
       "99999  generating reports, dashboards, and analytical...   \n",
       "\n",
       "                                              sql_prompt  \\\n",
       "0      What is the total volume of timber sold by eac...   \n",
       "1      List all the unique equipment types and their ...   \n",
       "2      How many marine species are found in the South...   \n",
       "3      What is the total trade value and average pric...   \n",
       "4      Find the energy efficiency upgrades with the h...   \n",
       "...                                                  ...   \n",
       "99995  Which programs had the highest volunteer parti...   \n",
       "99996  What is the number of fair-trade certified acc...   \n",
       "99997  Find the user with the longest workout session...   \n",
       "99998  How many space missions were completed by each...   \n",
       "99999  Determine the number of unique animal species ...   \n",
       "\n",
       "                                             sql_context  \\\n",
       "0      CREATE TABLE salesperson (salesperson_id INT, ...   \n",
       "1      CREATE TABLE equipment_maintenance (equipment_...   \n",
       "2      CREATE TABLE marine_species (name VARCHAR(50),...   \n",
       "3      CREATE TABLE trade_history (id INT, trader_id ...   \n",
       "4      CREATE TABLE upgrades (id INT, cost FLOAT, typ...   \n",
       "...                                                  ...   \n",
       "99995  CREATE TABLE programs (program_id INT, num_vol...   \n",
       "99996  CREATE TABLE products (product_id INT, product...   \n",
       "99997  CREATE TABLE workout_sessions (id INT, user_id...   \n",
       "99998  CREATE TABLE SpaceMissions (id INT, astronaut_...   \n",
       "99999  CREATE TABLE animal_population (id INT, animal...   \n",
       "\n",
       "                                                     sql  \\\n",
       "0      SELECT salesperson_id, name, SUM(volume) as to...   \n",
       "1      SELECT equipment_type, SUM(maintenance_frequen...   \n",
       "2      SELECT COUNT(*) FROM marine_species WHERE loca...   \n",
       "3      SELECT trader_id, stock, SUM(price * quantity)...   \n",
       "4      SELECT type, cost FROM (SELECT type, cost, ROW...   \n",
       "...                                                  ...   \n",
       "99995  SELECT program_id, (num_volunteers / total_par...   \n",
       "99996  SELECT COUNT(*) FROM products WHERE is_fair_tr...   \n",
       "99997  SELECT u.name, MAX(session_duration) as max_du...   \n",
       "99998  SELECT a.name, COUNT(sm.id) FROM Astronauts a ...   \n",
       "99999  SELECT COUNT(DISTINCT animal_species) AS uniqu...   \n",
       "\n",
       "                                         sql_explanation  \n",
       "0      Joins timber_sales and salesperson tables, gro...  \n",
       "1      This query groups the equipment_maintenance ta...  \n",
       "2      This query counts the number of marine species...  \n",
       "3      This query calculates the total trade value an...  \n",
       "4      The SQL query uses the ROW_NUMBER function to ...  \n",
       "...                                                  ...  \n",
       "99995  This query calculates the participation rate f...  \n",
       "99996  The query counts the number of fair-trade cert...  \n",
       "99997  The query joins the workout_sessions and users...  \n",
       "99998  This query calculates the number of space miss...  \n",
       "99999  This query calculates the number of unique ani...  \n",
       "\n",
       "[100000 rows x 11 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'synthetic_text_to_sql_train.snappy.parquet', 'test': 'synthetic_text_to_sql_test.snappy.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/gretelai/synthetic_text_to_sql/\" + splits[\"train\"])\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"feature-extraction\", model=\"microsoft/codebert-base\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: (1, 12, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "sql_query = \"SELECT name, age FROM users WHERE age > 30\"\n",
    "embeddings = pipe(sql_query)\n",
    "\n",
    "embedding_size = np.array(embeddings).shape\n",
    "print(f\"Embedding size: {embedding_size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FAISS as Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_sql_queries=0    SELECT salesperson_id, name, SUM(volume) as to...\n",
      "1    SELECT equipment_type, SUM(maintenance_frequen...\n",
      "2    SELECT COUNT(*) FROM marine_species WHERE loca...\n",
      "3    SELECT trader_id, stock, SUM(price * quantity)...\n",
      "4    SELECT type, cost FROM (SELECT type, cost, ROW...\n",
      "5    SELECT SUM(spending) FROM defense.eu_humanitar...\n",
      "6    SELECT SpeciesName, AVG(WaterTemp) as AvgTemp ...\n",
      "7    DELETE FROM Program_Outcomes WHERE program_id ...\n",
      "8    SELECT SUM(fare) FROM bus_routes WHERE route_n...\n",
      "9    SELECT AVG(Property_Size) FROM Inclusive_Housi...\n",
      "Name: sql, dtype: object, text_dict={0: 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;', 1: 'SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;', 2: \"SELECT COUNT(*) FROM marine_species WHERE location = 'Southern Ocean';\", 3: 'SELECT trader_id, stock, SUM(price * quantity) as total_trade_value, AVG(price) as avg_price FROM trade_history GROUP BY trader_id, stock;', 4: 'SELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1;', 5: 'SELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;', 6: 'SELECT SpeciesName, AVG(WaterTemp) as AvgTemp FROM SpeciesWaterTemp INNER JOIN FishSpecies ON SpeciesWaterTemp.SpeciesID = FishSpecies.SpeciesID WHERE MONTH(Date) = 2 GROUP BY SpeciesName;', 7: 'DELETE FROM Program_Outcomes WHERE program_id = 1002;', 8: \"SELECT SUM(fare) FROM bus_routes WHERE route_name = 'Green Line';\", 9: \"SELECT AVG(Property_Size) FROM Inclusive_Housing WHERE Inclusive = 'Yes';\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 25.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "d = 768  # 向量维度\n",
    "\n",
    "top_sql_queries = df.head(10)[\"sql\"]  # 假设字段名称是 sql_query\n",
    "\n",
    "# 使用一个字典来存储原文与其对应的索引\n",
    "text_dict = {i: top_sql_queries[i] for i in range(len(top_sql_queries))}\n",
    "print(f\"top_sql_queries={top_sql_queries}, text_dict={text_dict}\")\n",
    "\n",
    "index = faiss.IndexFlatL2(d)  # 使用 L2 距离\n",
    "\n",
    "for sql_query in tqdm(top_sql_queries):\n",
    "  embeddings = pipe(sql_query)\n",
    "  # 转换嵌入为 NumPy 数组并计算平均值（句子的整体 embedding）\n",
    "  embeddings = np.array(embeddings[0])  # 获取第一个元素，即每个 token 的 embedding\n",
    "  sentence_embedding = embeddings.mean(axis=0)  # 对所有 token 的 embedding 取平均\n",
    "  \n",
    "  index.add(np.expand_dims(sentence_embedding, axis=0).astype('float32'))  # 添加到 FAISS 索引中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to retrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (1, 125, 768)\n",
      "Shape of sentence embedding: (1, 768)\n",
      "Shape of query vector for FAISS: (1, 768)\n",
      "query=\n",
      "  SELECT    SUM(volume) AS total_volume,\n",
      "            salesperson_id,\n",
      "            name\n",
      "  FROM      timber_sales\n",
      "  JOIN      salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id\n",
      "  GROUP BY  salesperson_id,\n",
      "            name\n",
      "  ORDER BY  total_volume DESC;\n",
      "\n",
      "SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "SELECT trader_id, stock, SUM(price * quantity) as total_trade_value, AVG(price) as avg_price FROM trade_history GROUP BY trader_id, stock;\n",
      "SELECT SpeciesName, AVG(WaterTemp) as AvgTemp FROM SpeciesWaterTemp INNER JOIN FishSpecies ON SpeciesWaterTemp.SpeciesID = FishSpecies.SpeciesID WHERE MONTH(Date) = 2 GROUP BY SpeciesName;\n",
      "SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type;\n",
      "SELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经加载了数据并取得了前 1 条 SQL 查询\n",
    "xq = \"\"\"\n",
    "  SELECT    SUM(volume) AS total_volume,\n",
    "            salesperson_id,\n",
    "            name\n",
    "  FROM      timber_sales\n",
    "  JOIN      salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id\n",
    "  GROUP BY  salesperson_id,\n",
    "            name\n",
    "  ORDER BY  total_volume DESC;\n",
    "\"\"\"\n",
    "\n",
    "# 计算 SQL 查询的 embeddings\n",
    "xq_embeddings = pipe(xq)\n",
    "xq_embeddings = np.array(xq_embeddings)\n",
    "\n",
    "# 确保 embeddings 的形状为 (1, num_tokens, embedding_dim)\n",
    "print(f\"Shape of embeddings: {xq_embeddings.shape}\")  # 形状应该是 (1, 63, 768)\n",
    "\n",
    "# 对所有 token embeddings 取平均，得到句子的 embedding\n",
    "xq_sentence_embedding = xq_embeddings.mean(axis=1)  # 对维度 1 取平均，得到 (1, 768)\n",
    "\n",
    "# 确认句子的 embedding 形状，应该是 (768,)\n",
    "print(f\"Shape of sentence embedding: {xq_sentence_embedding.shape}\")  # 应该是 (768,)\n",
    "\n",
    "# 将句子的 embedding 转换为 (1, 768) 形式\n",
    "xq_sentence_vector = xq_sentence_embedding.astype('float32')  # 不需要 expand_dims，因为它已经是二维的\n",
    "\n",
    "# 确保索引的维度与 embedding 的维度匹配\n",
    "print(f\"Shape of query vector for FAISS: {xq_sentence_vector.shape}\")  # 应该是 (1, 768)\n",
    "\n",
    "# 查询索引，返回最相似的 5 个向量\n",
    "D, I = index.search(xq_sentence_vector, 5)\n",
    "\n",
    "print(f\"query={xq}\")\n",
    "# 打印最相似的向量的原文\n",
    "for idx in I[0]:\n",
    "    print(f\"{text_dict[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Try Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:23<00:00, 42.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 1000 points in 768D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "centroids shape before reshape: (7680,) objective=5793.7 imbalance=1.222 nsplit=0        \n",
      "聚类中心：\n",
      "[[-0.18499173  0.22511667  0.13517778 ... -0.54662466 -0.41021362\n",
      "   0.46190667]\n",
      " [-0.28051266  0.08221427  0.15986638 ... -0.44513115 -0.38535556\n",
      "   0.4418672 ]\n",
      " [-0.29708812  0.06968737  0.14393795 ... -0.46478567 -0.41295123\n",
      "   0.46540022]\n",
      " ...\n",
      " [-0.33051264  0.0879201   0.20769997 ... -0.47243038 -0.46141124\n",
      "   0.5047192 ]\n",
      " [-0.33388335  0.08140443  0.16814938 ... -0.49328902 -0.46472928\n",
      "   0.45314988]\n",
      " [-0.34249762  0.11567847  0.23195948 ... -0.5568997  -0.44863892\n",
      "   0.46856833]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 设置聚类的簇数\n",
    "k = 10  # 假设我们想要10个簇\n",
    "d = 768  # 向量维度\n",
    "\n",
    "top_sql_queries = df.head(1000)[\"sql\"]  # 假设字段名称是 sql_query\n",
    "\n",
    "clustering = faiss.Clustering(d, k)\n",
    "\n",
    "# 设置聚类的参数（例如，最大迭代次数）\n",
    "clustering.niter = 20  # 设置最大迭代次数\n",
    "clustering.max_points_per_centroid = 1000  # 每个质心的最大点数\n",
    "clustering.verbose = True  # 输出详细信息\n",
    "\n",
    "# 使用一个字典来存储原文与其对应的索引\n",
    "text_dict = {i: top_sql_queries[i] for i in range(len(top_sql_queries))}\n",
    "\n",
    "index = faiss.IndexHNSWFlat(d, 32)  # 使用 IndexFlatIP 距离\n",
    "\n",
    "vectors = []\n",
    "for sql_query in tqdm(top_sql_queries):\n",
    "  embeddings = pipe(sql_query)\n",
    "  # 转换嵌入为 NumPy 数组并计算平均值（句子的整体 embedding）\n",
    "  embeddings = np.array(embeddings[0])  # 获取第一个元素，即每个 token 的 embedding\n",
    "  sentence_embedding = embeddings.mean(axis=0)  # 对所有 token 的 embedding 取平均\n",
    "  \n",
    "  vector = np.expand_dims(sentence_embedding, axis=0).astype('float32')\n",
    "  vectors.append(vector)\n",
    "\n",
    "# 将所有向量堆叠成一个大的数组\n",
    "vectors = np.vstack(vectors)  # (num_samples, d)\n",
    "\n",
    "# 执行聚类训练\n",
    "clustering.train(vectors, index)  # 进行聚类训练\n",
    "\n",
    "# 获取聚类中心\n",
    "centroids = faiss.vector_float_to_array(clustering.centroids)  # 获取聚类中心\n",
    "print(f\"centroids shape before reshape: {centroids.shape}\")\n",
    "\n",
    "# 检查是否可以 reshape\n",
    "if centroids.shape[0] == k * d:\n",
    "    centroids = centroids.reshape(k, d)  # reshape 成 k 个簇中心，每个簇中心是 d 维\n",
    "    print(f\"聚类中心：\\n{centroids}\")\n",
    "else:\n",
    "    print(f\"聚类中心的形状不符合预期，实际大小为 {centroids.shape}\")\n",
    "\n",
    "# 添加向量到索引\n",
    "index.add(vectors)  # 聚类后添加向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "簇 0 的相邻 SQL 查询：\n",
      "\tSELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC; (距离: 0.0)\n",
      "\tSELECT establishment_year, COUNT(*) FROM marine_protected_areas GROUP BY establishment_year; (距离: 2.11226749420166)\n",
      "\tSELECT language, DATE_TRUNC('week', publish_date) as week, SUM(word_count) as total_word_count FROM Articles GROUP BY language, week ORDER BY week; (距离: 2.2348647117614746)\n",
      "\tSELECT MAX(revenue) FROM restaurants; (距离: 2.980682849884033)\n",
      "\tSELECT country, MIN(employee_count), MAX(employee_count) FROM SupplyChainEmployees WHERE year = 2020 GROUP BY country; (距离: 3.0246222019195557)\n",
      "\n",
      "\n",
      "簇 1 的相邻 SQL 查询：\n",
      "\tSELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type; (距离: 0.0)\n",
      "\tSELECT COUNT(*) FROM marine_species WHERE location = 'Southern Ocean'; (距离: 3.6753854751586914)\n",
      "\tSELECT departments.name, COUNT(*) AS num_employees FROM hr.departments JOIN hr.employees ON departments.id = employees.department_id GROUP BY departments.name ORDER BY num_employees DESC; (距离: 3.8941876888275146)\n",
      "\tSELECT sensor_type FROM IoT_Sensors WHERE location IN ('India', 'Germany'); (距离: 3.988931894302368)\n",
      "\tSELECT Building_Permits.Permit_ID, Building_Type, Issue_Date, Worker_Count, Year FROM Building_Permits INNER JOIN Residential_Buildings ON Building_Permits.Building_Type = Residential_Buildings.Building_Type INNER JOIN Labor_Statistics ON Building_Permits.Permit_ID = Labor_Statistics.Permit_ID WHERE Issue_Date BETWEEN '2018-01-01' AND '2020-12-31'; (距离: 3.9909965991973877)\n",
      "\n",
      "\n",
      "簇 2 的相邻 SQL 查询：\n",
      "\tSELECT COUNT(*) FROM marine_species WHERE location = 'Southern Ocean'; (距离: 0.0)\n",
      "\tSELECT COUNT(*) FROM hospitals WHERE state = 'California'; (距离: 2.6889004707336426)\n",
      "\tSELECT MIN(age) FROM rural_clinics WHERE country = 'Mexico'; (距离: 2.992401599884033)\n",
      "\tSELECT (SUM(CASE WHEN Service = 'Education' THEN Budget ELSE 0 END) / SUM(Budget)) * 100 FROM Budget WHERE Year IN (2020, 2021); (距离: 3.012735366821289)\n",
      "\tSELECT SpeciesName, AVG(WaterTemp) as AvgTemp FROM SpeciesWaterTemp INNER JOIN FishSpecies ON SpeciesWaterTemp.SpeciesID = FishSpecies.SpeciesID WHERE MONTH(Date) = 2 GROUP BY SpeciesName; (距离: 3.2098913192749023)\n",
      "\n",
      "\n",
      "簇 3 的相邻 SQL 查询：\n",
      "\tSELECT trader_id, stock, SUM(price * quantity) as total_trade_value, AVG(price) as avg_price FROM trade_history GROUP BY trader_id, stock; (距离: 0.0)\n",
      "\tSELECT vehicle_name, safety_rating FROM (SELECT vehicle_name, safety_rating, RANK() OVER (ORDER BY safety_rating DESC) as safety_rank FROM auto_show WHERE vehicle_name LIKE '%Autonomous%') AS auton_ranks WHERE safety_rank = 1; (距离: 1.9660760164260864)\n",
      "\tSELECT item, sodium FROM MenuItems WHERE popularity > 70 ORDER BY sodium DESC; (距离: 2.0591254234313965)\n",
      "\tDELETE FROM accommodations WHERE student_id = 10 AND YEAR(accommodation_date) = 2023; (距离: 2.0697054862976074)\n",
      "\tSELECT COUNT(VolunteerID) FROM Volunteers WHERE Community IN ('Women in Tech', 'Minorities in STEM') AND Year = 2019; (距离: 2.4783167839050293)\n",
      "\n",
      "\n",
      "簇 4 的相邻 SQL 查询：\n",
      "\tSELECT type, cost FROM (SELECT type, cost, ROW_NUMBER() OVER (ORDER BY cost DESC) as rn FROM upgrades) sub WHERE rn = 1; (距离: 0.0)\n",
      "\tSELECT COUNT(*) FROM MaintenanceRequests WHERE country = 'Japan' AND request_date >= DATE_SUB(CURDATE(), INTERVAL 12 MONTH); (距离: 2.9295053482055664)\n",
      "\tSELECT DISTINCT Type FROM Hospitals WHERE Type != 'Mobile Clinic'; (距离: 3.115157127380371)\n",
      "\tSELECT h.name habitat, SUM(t.biomass) total_biomass FROM trees t JOIN tree_habitat_associations tha ON t.id = tha.tree_id JOIN wildlife_habitats h ON tha.habitat_id = h.id JOIN tree_types tt ON t.tree_type_id = tt.id WHERE tt.name = 'Deciduous' GROUP BY h.name; (距离: 3.3279495239257812)\n",
      "\tSELECT EXTRACT(DOW FROM publication_date) AS day_of_week, COUNT(*) AS count FROM articles WHERE EXTRACT(YEAR FROM publication_date) = 2022 GROUP BY day_of_week; (距离: 3.6455459594726562)\n",
      "\n",
      "\n",
      "簇 5 的相邻 SQL 查询：\n",
      "\tSELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021; (距离: 0.0)\n",
      "\tSELECT AVG(Vehicle.city_traffic_speed) FROM Vehicle INNER JOIN SafetyTesting ON Vehicle.id = SafetyTesting.vehicle_id WHERE Vehicle.is_autonomous = false; (距离: 3.8590190410614014)\n",
      "\tSELECT CityName, Policy, MAX(Impact) FROM PolicyImpact GROUP BY CityName HAVING COUNT(DISTINCT Policy) = 2; (距离: 4.029242038726807)\n",
      "\tSELECT AVG(Rainfall) as AvgAnnualRainfall, SUM(MaintenanceCost) as TotalAnnualMaintenanceCost  FROM BridgeRainfall  WHERE Region = 'Northwest' AND Year >= YEAR(CURRENT_DATE) - 5 GROUP BY Year; (距离: 4.360701560974121)\n",
      "\tSELECT SUM(amount) as total_donation FROM donations WHERE donor IN ('Sophia Lee', 'Ali Al-Khaleej') AND YEAR(donation_date) = 2022; (距离: 4.367681503295898)\n",
      "\n",
      "\n",
      "簇 6 的相邻 SQL 查询：\n",
      "\tSELECT SpeciesName, AVG(WaterTemp) as AvgTemp FROM SpeciesWaterTemp INNER JOIN FishSpecies ON SpeciesWaterTemp.SpeciesID = FishSpecies.SpeciesID WHERE MONTH(Date) = 2 GROUP BY SpeciesName; (距离: 0.0)\n",
      "\tSELECT COUNT(*) FROM buildings WHERE region = 'Southern' AND year = 2020 AND type = 'Green'; (距离: 2.2949583530426025)\n",
      "\tSELECT gender, COUNT(*) FROM vaccinations GROUP BY gender; (距离: 2.4863569736480713)\n",
      "\tSELECT SUM(horsepower) FROM green_vehicles WHERE make = 'Tesla' OR make = 'Rivian'; (距离: 2.4953575134277344)\n",
      "\tSELECT make, model, MAX(horsepower) FROM geneva_motor_show WHERE year = 2021; (距离: 2.5321218967437744)\n",
      "\n",
      "\n",
      "簇 7 的相邻 SQL 查询：\n",
      "\tDELETE FROM Program_Outcomes WHERE program_id = 1002; (距离: 0.0)\n",
      "\tSELECT state, COUNT(*) FROM genetics_stats.research_projects GROUP BY state; (距离: 3.399186611175537)\n",
      "\tSELECT project_name, risk_level FROM DefenseProjects WHERE start_date >= '2019-01-01' AND start_date <= '2019-12-31' AND risk_level IS NOT NULL; (距离: 3.61454439163208)\n",
      "\tSELECT R.Cuisine, SUM(M.Sales) as TotalRevenue, COUNT(DISTINCT R.RestaurantID) as RestaurantCount FROM Restaurants R INNER JOIN MenuItems M ON R.RestaurantID = M.RestaurantID GROUP BY R.Cuisine; (距离: 3.649418830871582)\n",
      "\tSELECT title FROM movie WHERE genre = 'Animation' ORDER BY viewers DESC LIMIT 3; (距离: 3.7281532287597656)\n",
      "\n",
      "\n",
      "簇 8 的相邻 SQL 查询：\n",
      "\tSELECT SUM(fare) FROM bus_routes WHERE route_name = 'Green Line'; (距离: 0.0)\n",
      "\tUPDATE Building SET certification = 'LEED Gold' WHERE city = 'Chicago'; (距离: 2.747807025909424)\n",
      "\tSELECT mode, SUM(ridership) FROM TOPublicTransportation WHERE date = '2022-03-01' GROUP BY mode; (距离: 2.7987544536590576)\n",
      "\tSELECT SUM(Sales) FROM PharmaSales WHERE DrugName = 'DrugC' AND Year = 2016; (距离: 2.8536322116851807)\n",
      "\tSELECT * FROM genetic_research WHERE community = 'Indigenous'; (距离: 2.979227066040039)\n",
      "\n",
      "\n",
      "簇 9 的相邻 SQL 查询：\n",
      "\tSELECT AVG(Property_Size) FROM Inclusive_Housing WHERE Inclusive = 'Yes'; (距离: 0.0)\n",
      "\t索引 1005 对应的原文不存在！\n",
      "\tSELECT Policyholders.PolicyholderID, Policyholders.LastClaimDate, COUNT(Claims.ClaimID) AS NumberOfClaims FROM Policyholders LEFT JOIN Claims ON Policyholders.PolicyholderID = Claims.PolicyholderID WHERE Claims.ClaimDate BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 365 DAY) AND CURRENT_DATE() GROUP BY Policyholders.PolicyholderID HAVING COUNT(Claims.ClaimID) > 1; (距离: 2.8857295513153076)\n",
      "\tSELECT farm_id, AVG(soil_moisture) AS avg_moisture FROM farm_soil_moisture WHERE timestamp >= NOW() - INTERVAL '1 year' GROUP BY farm_id ORDER BY avg_moisture DESC LIMIT 3; (距离: 3.0470151901245117)\n",
      "\tDELETE FROM carbon_offset_initiatives WHERE project_name = 'Green Spaces' AND state = 'New York'; (距离: 3.2045586109161377)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 获取每个簇的相邻 SQL 查询\n",
    "k_nearest_neighbors = 5  # 每个簇返回 5 个最相似的向量（即 SQL 查询）\n",
    "for i in range(k):\n",
    "    # 找到与第 i 个簇中心最接近的向量\n",
    "    centroid_vector = np.expand_dims(centroids[i], axis=0).astype('float32')  # 当前簇中心\n",
    "    distances, indices = index.search(centroid_vector, k_nearest_neighbors)  # 查找最近的k个向量\n",
    "    \n",
    "    print(f\"簇 {i} 的相邻 SQL 查询：\")\n",
    "    for j in range(k_nearest_neighbors):\n",
    "        closest_index = indices[0][j]  # 获取最近的向量索引\n",
    "        \n",
    "        # 检查索引是否在 text_dict 中\n",
    "        if closest_index in text_dict:\n",
    "            closest_text = text_dict[closest_index]  # 获取该向量的原文\n",
    "            print(f\"\\t{closest_text} (距离: {distances[0][j]})\")\n",
    "        else:\n",
    "            print(f\"\\t索引 {closest_index} 对应的原文不存在！\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
